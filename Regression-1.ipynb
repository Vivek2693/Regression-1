{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87627e7d",
   "metadata": {},
   "source": [
    "### Q1. Simple Linear Regression vs. Multiple Linear Regression:\n",
    "\n",
    "Simple Linear Regression: In simple linear regression, there is only one independent variable (predictor variable) used to predict the dependent variable. The relationship between the independent variable and the dependent variable is assumed to be linear. An example of simple linear regression would be predicting a student's exam score (dependent variable) based on the number of hours they studied (independent variable).\n",
    "Multiple Linear Regression: In multiple linear regression, there are two or more independent variables used to predict the dependent variable. The relationship between the independent variables and the dependent variable is assumed to be linear. An example of multiple linear regression would be predicting a house price (dependent variable) based on various features such as s### Quare footage, number of bedrooms, and location.\n",
    "### Q2. Assumptions of Linear Regression and Checking Assumptions:\n",
    "\n",
    "Assumptions: The main assumptions of linear regression include linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of errors.\n",
    "Checking Assumptions: These assumptions can be checked using various diagnostic plots such as residual plots, normal probability plots, and scatterplots. Additionally, statistical tests such as the Durbin-Watson test for autocorrelation and the Breusch-Pagan test for heteroscedasticity can be performed.\n",
    "### Q3. Interpretation of Slope and Intercept:\n",
    "\n",
    "Slope (Coefficient): The slope represents the change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant. For example, in a simple linear regression predicting exam scores based on study hours, the slope represents the expected change in exam score for each additional hour studied.\n",
    "Intercept: The intercept represents the value of the dependent variable when all independent variables are zero. In the context of the example, the intercept represents the expected exam score when a student did not study at all.\n",
    "### Q4. Gradient Descent:\n",
    "\n",
    "Concept: Gradient descent is an optimization algorithm used to minimize the error (or cost) function in machine learning models. It works by iteratively updating the parameters (coefficients) of the model in the direction of the steepest descent of the cost function.\n",
    "Usage: In machine learning, gradient descent is used to train models such as linear regression, logistic regression, and neural networks by finding the optimal parameters that minimize the difference between the predicted and actual values.\n",
    "### Q5. Multiple Linear Regression Model:\n",
    "\n",
    "Description: Multiple linear regression is an extension of simple linear regression where there are two or more independent variables used to predict the dependent variable. The relationship between the independent variables and the dependent variable is assumed to be linear.\n",
    "Difference from Simple Linear Regression: The main difference from simple linear regression is the presence of multiple independent variables, allowing for a more complex model that accounts for multiple factors influencing the dependent variable.\n",
    "### Q6. Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Concept: Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated with each other. This can lead to unstable estimates of the coefficients and make it difficult to interpret the individual effects of the independent variables on the dependent variable.\n",
    "Detection and Addressing: Multicollinearity can be detected using correlation matrices or variance inflation factors (VIFs). To address multicollinearity, you can consider removing one of the correlated variables, combining the correlated variables into a single variable, or using regularization techni### Ques such as ridge regression.\n",
    "### Q7. Polynomial Regression Model:\n",
    "\n",
    "Description: Polynomial regression is a form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial. This allows for capturing non-linear relationships between the variables.\n",
    "Difference from Linear Regression: The main difference from linear regression is that polynomial regression can capture non-linear relationships between the variables by fitting a curve to the data rather than a straight line.\n",
    "### Q8. Advantages and Disadvantages of Polynomial Regression:\n",
    "\n",
    "Advantages: Polynomial regression can capture complex relationships between variables that linear regression cannot. It can provide a better fit to the data when the relationship is non-linear.\n",
    "Disadvantages: Polynomial regression can lead to overfitting, especially with higher degrees of polynomial. It may also be computationally expensive and harder to interpret compared to linear regression.\n",
    "Preference for Polynomial Regression: Polynomial regression is preferred when there is evidence of non-linear relationships between the variables and when higher accuracy is desired, even at the expense of increased complexity and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6035e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
